# Test model 
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import json
from sklearn.metrics import classification_report, confusion_matrix

# Load mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
model_path = "/content/drive/MyDrive/dataset/hash_algorithm_model.h5"
model = tf.keras.models.load_model(model_path)

# Load táº­p kiá»ƒm tra
test_data_path = "/content/drive/MyDrive/dataset/hash_algorithm_test_dataset.csv"
df_test = pd.read_csv(test_data_path)

df_test["Byte Distribution"] = df_test["Byte Distribution"].apply(lambda x: json.loads(x) if isinstance(x, str) else x)

# MÃ£ hÃ³a cá»™t "Optimal Hash Algorithm" thÃ nh sá»‘
hash_algorithms = ["SHA3-512", "BLAKE3", "SHA-256", "Keccak-256"]
df_test["Optimal Hash Algorithm"] = df_test["Optimal Hash Algorithm"].apply(lambda x: hash_algorithms.index(x))

# MÃ£ hÃ³a cá»™t "Datatype" thÃ nh sá»‘
data_type = ["Random Bytes", "Blockchain TX", "JSON", "Image", "Text"]
df_test["Data Type"] = df_test["Data Type"].apply(lambda x: data_type.index(x) if x in data_type else -1)


# Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘áº§u vÃ o
X_test = df_test.drop(columns=["Optimal Hash Algorithm"])
y_test = df_test["Optimal Hash Algorithm"]

X_test["Byte Distribution"] = X_test["Byte Distribution"].apply(lambda x: np.array(x) if isinstance(x, list) else np.zeros(256))

byte_distribution_expanded = pd.DataFrame(X_test["Byte Distribution"].to_list(), index=X_test.index)
X_test = X_test.drop(columns=["Byte Distribution"]).join(byte_distribution_expanded)

X_test.columns = X_test.columns.astype(str)

# Chuáº©n hÃ³a dá»¯ liá»‡u
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_test = scaler.fit_transform(X_test)

# Dá»± Ä‘oÃ¡n trÃªn táº­p kiá»ƒm tra
y_pred_prob = model.predict(X_test)
y_pred = np.argmax(y_pred_prob, axis=1)

# In bÃ¡o cÃ¡o hiá»‡u suáº¥t
print("ğŸ“Š BÃ¡o cÃ¡o Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh:")
print(classification_report(y_test, y_pred, target_names=["SHA3-512", "BLAKE3", "SHA-256", "Keccak-256"]))

# Biá»ƒu Ä‘á»“ "Äá»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n theo thuáº­t toÃ¡n bÄƒm"
# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# TÃ­nh sá»‘ lÆ°á»£ng máº«u Ä‘Ãºng trÃªn tá»•ng sá»‘ máº«u cá»§a má»—i thuáº­t toÃ¡n
accuracy_per_class = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)

# Váº½ biá»ƒu Ä‘á»“ Ä‘Æ°á»ng
plt.figure(figsize=(8, 5))
plt.plot(hash_algorithms, accuracy_per_class, marker='o', linestyle='-', color='b', label="Äá»™ chÃ­nh xÃ¡c")

# Äá»‹nh dáº¡ng biá»ƒu Ä‘á»“
plt.xlabel("Thuáº­t toÃ¡n bÄƒm")
plt.ylabel("Äá»™ chÃ­nh xÃ¡c")
plt.title("Äá»™ chÃ­nh xÃ¡c dá»± Ä‘oÃ¡n theo thuáº­t toÃ¡n bÄƒm")
plt.ylim(0, 1)  # GiÃ¡ trá»‹ trong khoáº£ng 0-1
plt.grid(True)
plt.legend()
plt.show()

# Biá»ƒu Ä‘á»“ "Táº§n suáº¥t dá»± Ä‘oÃ¡n theo thuáº­t toÃ¡n bÄƒm"
import collections

# Äáº¿m sá»‘ lÆ°á»£ng dá»± Ä‘oÃ¡n má»—i thuáº­t toÃ¡n
pred_counts = collections.Counter(y_pred)

# Chuyá»ƒn Ä‘á»•i thÃ nh danh sÃ¡ch theo thá»© tá»± thuáº­t toÃ¡n bÄƒm
pred_frequencies = [pred_counts[i] for i in range(len(hash_algorithms))]

# Váº½ biá»ƒu Ä‘á»“ Ä‘Æ°á»ng
plt.figure(figsize=(8, 5))
plt.plot(hash_algorithms, pred_frequencies, marker='s', linestyle='-', color='g', label="Táº§n suáº¥t dá»± Ä‘oÃ¡n")

# Äá»‹nh dáº¡ng biá»ƒu Ä‘á»“
plt.xlabel("Thuáº­t toÃ¡n bÄƒm")
plt.ylabel("Sá»‘ láº§n Ä‘Æ°á»£c dá»± Ä‘oÃ¡n")
plt.title("Táº§n suáº¥t dá»± Ä‘oÃ¡n theo thuáº­t toÃ¡n bÄƒm")
plt.grid(True)
plt.legend()
plt.show()
